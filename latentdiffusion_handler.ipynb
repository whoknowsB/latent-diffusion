{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os\n",
    "import numpy as np\n",
    "import base64\n",
    "\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torchvision.utils import make_grid\n",
    "from einops import rearrange\n",
    "\n",
    "from tqdm import tqdm, trange\n",
    "from huggingface_hub import hf_hub_download\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "\n",
    "from ts.context import Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_from_config(config, ckpt, device, verbose=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "OPT = argparse.Namespace(\n",
    "    ddim_steps = 50, # 200\n",
    "    ddim_eta = 0, # 0\n",
    "    n_iter = 1, # 1\n",
    "    W = 256, # 256\n",
    "    H = 256, # 256\n",
    "    n_samples = 3, # 4\n",
    "    scale = 5.0, # 5.0\n",
    "    plms = True,\n",
    "    format = 'JPEG'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.initialized = False\n",
    "        self.device = None\n",
    "\n",
    "    def initialize(self, context):\n",
    "\n",
    "        #  load the model\n",
    "        self.manifest = context.manifest\n",
    "        \n",
    "\n",
    "        properties = context.system_properties\n",
    "        model_dir = properties.get(\"model_dir\")\n",
    "        self.device = torch.device(\"cuda:\" + str(properties.get(\"gpu_id\")) if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # Read model serialize/pt file\n",
    "        serialized_file = self.manifest['model']['serializedFile']\n",
    "\n",
    "        model_path = os.path.join(model_dir, serialized_file)\n",
    "        config_path = os.path.join(model_dir, \"config.yaml\")\n",
    "\n",
    "        if not os.path.isfile(model_path):\n",
    "            raise RuntimeError(\"Missing the model.pt file\")\n",
    "        if not os.path.isfile(config_path):\n",
    "            raise RuntimeError(\"Missing the config.yaml file\")\n",
    "\n",
    "        config = OmegaConf.load(config_path)\n",
    "        self.model = load_model_from_config(config, model_path, self.device, verbose=True)\n",
    "        self.sampler = PLMSSampler(self.model) if OPT.plms else DDIMSampler(self.model)\n",
    "        self.initialized = True\n",
    "\n",
    "    def handle(self, data, context):\n",
    "        prompt = str(data[0])\n",
    "        uc = None\n",
    "        all_samples=[]\n",
    "\n",
    "        if OPT.scale != 1.0:\n",
    "            uc = self.model.get_learned_conditioning(OPT.n_samples * [\"\"])\n",
    "        for n in trange(OPT.n_iter, desc=\"Sampling\"):\n",
    "            c = self.model.get_learned_conditioning(OPT.n_samples * [prompt])\n",
    "            shape = [4, OPT.H//8, OPT.W//8]\n",
    "            samples_ddim, _ = self.sampler.sample(S=OPT.ddim_steps,\n",
    "                                                conditioning=c,\n",
    "                                                batch_size=OPT.n_samples,\n",
    "                                                shape=shape,\n",
    "                                                verbose=False,\n",
    "                                                unconditional_guidance_scale=OPT.scale,\n",
    "                                                unconditional_conditioning=uc,\n",
    "                                                eta=OPT.ddim_eta)\n",
    "\n",
    "            x_samples_ddim = self.model.decode_first_stage(samples_ddim)\n",
    "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, min=0.0, max=1.0)\n",
    "\n",
    "            #for x_sample in x_samples_ddim:\n",
    "            #    x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "            #    x_sample = x_sample.astype(np.uint8)\n",
    "            #    Image.fromarray(x_sample).save(os.path.join('outputs/samples', f\"{base_count:04}.png\"))\n",
    "            #    base_count += 1\n",
    "            all_samples.append(x_samples_ddim)\n",
    "\n",
    "        grid = torch.stack(all_samples, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "\n",
    "        #grid = make_grid(grid, nrow=OPT.n_samples)\n",
    "        #grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        #grid = Image.fromarray(grid.astype(np.uint8))\n",
    "        #buffer = BytesIO()\n",
    "        #grid.save(buffer, format = OPT.format)\n",
    "        #encode = base64.b64encode(buffer.getvalue()).decode()\n",
    "        #return [encode]\n",
    "        \n",
    "        images = []\n",
    "        for image in grid:\n",
    "            image = 255. * rearrange(image, 'c h w -> h w c').cpu().numpy()\n",
    "            image = Image.fromarray(image.astype(np.uint8))\n",
    "            images.append(image)\n",
    "\n",
    "        encodes = []\n",
    "        for image in images:\n",
    "            buffer = BytesIO()\n",
    "            image.save(buffer, format = OPT.format)\n",
    "            encode = base64.b64encode(buffer.getvalue()).decode()\n",
    "            encodes.append(encode)\n",
    "\n",
    "        return [encodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#context = Context(\n",
    "#    'latentdiffusion', \n",
    "#    '/home/callmeb/Documents/reply-hackathon-2022/latent-diffusion/models/ldm/text2img-large',\n",
    "#    {'model':{'serializedFile': 'model.ckpt'}},\n",
    "#    1,\n",
    "#    0,\n",
    "#    'server_version_0'\n",
    "#    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mh = ModelHandler()\n",
    "#mh.initialize(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "#data = [{'body': 'a cat made of wool'}]\n",
    "#out = mh.handle(data, context)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for o in out:\n",
    "#    im_bytes = base64.b64decode(o.encode())  \n",
    "#    im_file = BytesIO(im_bytes)\n",
    "#    img = Image.open(im_file)\n",
    "#    display(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch-model-archiver --model-name latentdiffusion \\\n",
    "#                     --version 1.0 \\\n",
    "#                     --serialized-file /home/callmeb/Documents/reply-hackathon-2022/latent-diffusion/models/ldm/text2img-large/model.ckpt \\\n",
    "#                     --handler latentdiffusion_handler.py \\\n",
    "#                     --extra-files /home/callmeb/Documents/reply-hackathon-2022/latent-diffusion/models/ldm/text2img-large/config.yaml \\\n",
    "#                     --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchserve --start --models latentdiffusion=latentdiffusion.mar  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv latentdiffusion.mar model_store/  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# curl --location --request POST 'http://127.0.0.1:8080/predictions/latentdiffusion?data=a dog with funny blue hat'"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2fcf5ccbe1eaba4c605d02382f7a59b3e8637f9003b00fb772da54a35d06fa47"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 ('ldm')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
